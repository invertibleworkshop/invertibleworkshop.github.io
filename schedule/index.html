<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Dustin Tran, edited by Chris Cremer & Chin-Wei Huang">
  <link rel="shortcut icon" href="../img/favicon.ico" type="image/x-icon">
  <title>Invertible Neural Nets and Normalizing Flows - Call for Papers</title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="../css/main.css">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->
</head>





<body>
  <div class="container">

    <div class="row" style="padding:20px">

      <div class="col-xs-3 col-sm-3 col-md-3">
        <img src="../img/innf_logo.gif" style="height:90px" hspace="40" vspace="30" >
      </div>

      <div class="col-xs-12 col-sm-12 col-md-9">
        <div class="row" style="margin-bottom:-10px;">
          <h1 align="center"> INNF+ 2021 </h1>
          <h3 align="center"> ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models</h3>
          <hr>
          <!-- <br> -->
          <!-- <br> -->
          <!--           <p class="lead">
            December 2, 2018<br>
            <a href="https://goo.gl/maps/GZ2CkVfpFhL2">Le 1000 Conference Center</a><br>
            <a href="https://goo.gl/maps/GZ2CkVfpFhL2">1000 Rue de la Gauchetière Ouest</a><br>
            <a href="https://goo.gl/maps/GZ2CkVfpFhL2">Montréal, QC H3B 0A2, Canada</a><br>
          </p> -->
        </div>
      </div>





      <div class="col-xs-12 col-sm-3 col-md-3" id="sidebar" role="navigation" style="margin-top:-30px;">
        <hr>

        <ul class="nav nav-pills nav-stacked">
          <li><a href="../index.html">Home</a></li>
          <li><a href="../schedule/index.html">Schedule</a></li>
          <li><a href="../call/index.html">Call for Papers</a></li>
<!--          <li><a href="../reviewer_instructions/index.html">Reviewer Instructions</a></li>-->
          <li><a href="../author_instructions/index.html">Author Instructions</a></li>
          <li><a href="../how_it_works/index.html">How It Works</a></li>
          <li><a href="../accepted_papers/index.html">Accepted Papers</a></li>
          <li><a href="../invited_speakers/index.html">Invited Speakers</a></li>
        </ul>

        <hr>

        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Organizers</li>
          <li><a href="https://chinweihuang.com/">Chin-Wei Huang</a></li>
          <li><a href="https://mila.quebec/en/person/david-scott-krueger/">David Krueger</a></li>
          <li><a href="https://research.google/people/RiannevandenBerg/">Rianne van den Berg</a></li>
          <li><a href="https://gpapamak.github.io/">George Papamakarios</a></li>
          <li><a href="http://rtqichen.com">Ricky Chen</a></li>
          <li><a href="https://danilorezende.com/">Danilo Rezende</a></li>
        </ul>

        <hr>
      </div>






      <div class="col-xs-12 col-sm-9 col-md-9">
<!--        <hr>-->
        <div class="row">
<!--          <br>-->


          <h3>Schedule</h3>
          <h4>For a more detailed schedule, see the <a href="https://icml.cc/virtual/2021/workshop/8360" target="_blank">video stream link</a> (registration needed)</strong></li>.</h4>
          For all the videos of accepted papers, please visit <a href="../accepted_papers/index.html">the accepted papers page</a>.
          <br>
          (Times are local; <span id="timezone"></span>)
          <table class="table" style="margin-bottom:0px;">
            <tbody>
              <tr>
                <td width="18%"><span id="event_0"></span></td>
                <td width="18%"> </td>
                <td>Opening Remarks</br> </td>
              </tr>
              <tr>
                <td width="18%"><span id="event_1"></span></td>
                <td width="18%"><a href="http://csml.stats.ox.ac.uk/people/lelan/" target="_blank">Charline Le Lan<br>(Oxford)</a></td>
                <td> Invited Talk: <strong> On the use of density models for anomaly detection </strong> <br>
<!--                [<a href="https://slideslive.com/38930808/survae-flows-unifying-vaes-and-flows-into-one-framework">video</a>] </td>-->
                <button type="button" class="collapsible">Abstract</button>
                <div class="content">
                  Thanks to the tractability of their likelihood, some deep generative models show promise for seemingly straightforward but important applications like anomaly detection. However, the likelihood values empirically attributed to anomalies conflict with the expectations these proposed applications suggest.
                  This talk will review some of these density-based anomaly detection methods that have widely been used in the machine learning literature and question the expectation that density estimation should always enable anomaly detection.
                  In particular, we will examine the extent of the issues that can arise from these practices and look at some practical consequences.
                  Finally, the talk will also cover some promising directions for reliably detecting anomalies through density, in particular highlighting the importance of prior knowledge.
                  This project was joint work with Laurent Dinh.
                </div>
              </tr>
              <tr>
                <td><span id="event_2"></span></td>
                <td><a href="http://yingzhenli.net/home/en/" target="_blank">Yingzhen Li<br>(ICL)</a></td>
                <td> Invited Talk: <strong> Inference with scores: slices, diffusions and flows </strong> <br>
                <button type="button" class="collapsible">Abstract</button>
                <div class="content">
                  In this talk I will discuss our recent efforts on developing Stein's method for approximate inference and model learning.
                  I will start from an introduction of the score matching and Stein discrepancy, with a comparison to KL divergence based approaches.
                  Then I will discuss our recent works that tries to address the curse of dimensionality issues in existing Stein discrepancies.
                  The idea is based on slicing, and an important step within the approach is to measure the score difference in a different basis of $\mathbb{R}^d$.
                  Lastly we extend the basis modification idea to measuring score difference with local basis, and discuss an on-going work that aims to connect this approach with normalising flows.
                  This talk will also feature Wenbo Gong, a student collaborator with me on theory & applications of Stein’s method.
                </div>
              </tr>
              <tr>
                <td><span id="event_3"></span></td>
                <td></td>
                <td>Poster Spotlights I<br></br></td>
              </tr>
              <tr>
                <td><span id="event_4"></span></td>
                <td></td>
                <td>
                  Poster Session I <br>
                  <a href="https://eventhosts.gather.town/0fH1RU147QI1cqPq/innf-poster-room-1" target="_blank">Poster Room 1</a> -
                <a href="https://eventhosts.gather.town/Q6X3qZkT5TMp3HPu/innf-poster-room-2" target="_blank">Poster Room 2</a> -
                <a href="https://docs.google.com/spreadsheets/d/1l1hA6IyEDLkzNMQuO2BLtsLWAI05dEC0R5lxNPJriMY/edit#gid=2135000399">Presenting papers</a></td>
                </td>
              </tr>
              <tr>
                <td><span id="event_5"></span></td>
                <td><a href="https://physics.mit.edu/faculty/phiala-shanahan/" target="_blank">Phiala Shanahan<br>(MIT)</a> </td>
                <td>Invited Talk: <strong> Flow models for theoretical particle and nuclear physics </strong>  <br>
                <button type="button" class="collapsible">Abstract</button>
                <div class="content">
                  I will discuss opportunities for machine learning, in particular approaches based on normalizing flows, to accelerate first-principles lattice quantum field theory calculations in particle and nuclear physics.
                  Particular challenges in this context include incorporating complex (gauge) symmetries into model architectures, and scaling models to the large number of degrees of freedom of state-of-the-art numerical studies.
                  I will show the results of proof-of-principle studies that demonstrate that sampling from generative models can be orders of magnitude more efficient than traditional Hamiltonian/hybrid Monte Carlo approaches in this context.
                </div>

              </tr>
              <tr>
                <td><span id="event_6"></span></td>
                <td><a href="https://mbrubake.github.io/" target="_blank">Marcus Brubaker<br>(York)</a></td>
                <td>Invited Talk: <strong> Wavelet Flow: Fast Training of High Resolution Normalizing Flows </strong> <br>
                <button type="button" class="collapsible">Abstract</button>
                <div class="content">
                  This talk will introduce Wavelet Flow, a novel normalizing flow architecture which explicitly represents the scale-space structure of signals in the architecture of the normalizing flow through the use of wavelets.
                  The result is a generative model which automatically includes models of images at resolutions small than that used for training and is able to perform super-resolution with not additional effort.
                  Further, because of the structure of the architecture, each scale can be trained completely independently, leading to significant improvements in training efficiency and enabling the first reported normalizing flow model for 1024x1024 resolution images.
                  This project is joint work with Jason Yu and Kosta Derpanis.
                </div>
                </td>
              </tr>
              <tr>
                <td><span id="event_7"></span></td>
                <td></td>
                <td> Break  <br></br> </td>
              </tr>
              <tr>
                <td><span id="event_8"></span></td>
                <td><a href="https://cs.stanford.edu/~ermon/" target="_blank">Stefano Ermon<br>(Stanford)</a></td>
                <td>Invited Talk: <strong> Maximum Likelihood Training of Score-Based Diffusion Models </strong> <br>
                <button type="button" class="collapsible">Abstract</button>
                <div class="content">
                  Existing generative models are typically based on explicit representations of probability distributions (e.g., autoregressive or VAEs) or implicit sampling procedures (e.g., GANs).
                  We propose an alternative approach based on modeling directly the vector field of gradients of the data distribution (scores).
                  Our framework allows flexible architectures, requires no sampling during training or the use of adversarial training methods.
                  Additionally, score-based generative models enable exact likelihood evaluation through connections with normalizing flows.
                  We produce samples comparable to GANs, achieving new state-of-the-art inception scores, and competitive likelihoods on image datasets.
                </div>
                </td>
              </tr>
              <tr>
                <td><span id="event_9"></span></td>
                <td>Ann-Kathrin Dombrowski<br></td>
                <td>Contributed Talk I: <strong> Diffeomorphic Explanations with Normalizing Flows </strong><br>
                <button type="button" class="collapsible">Abstract</button>
                <div class="content">
                  Normalizing flows are diffeomorphisms which are parameterized by neural networks.
                  As a result, they can induce coordinate transformations in the tangent space of the data manifold.
                  In this work, we demonstrate that such transformations can be used to generate interpretable explanations for decisions of neural networks.
                  More specifically, we perform gradient ascent in the base space of the flow to generate counterfactuals which are classified with great confidence as a specified target class.
                  We analyze this generation process theoretically using Riemannian differential geometry and establish a rigorous theoretical connection between gradient ascent on the data manifold and in the base space of the flow.
                </div>
              </tr>
              <tr>
                <td><span id="event_10"></span></td>
                <td><a href="https://mnick.github.io/" target="_blank">Maximilian Nickel<br>(Facebook)</a></td>
                <td>Invited Talk: <strong> Modeling Spatio-Temporal Events via Normalizing Flows </strong> <br>
              </tr>

              <tr>
                <td><span id="event_11"></span></td>
                <td><a href="https://openai.com/blog/authors/aditya/" target="_blank">Aditya Ramesh<br>(OpenAI)</a></td>
                <td>Invited Talk: <strong> TBA </strong> <br>
              </tr>
              <tr>
                <td><span id="event_12"></span></td>
                <td>Marylou Gabrié<br></td>
                <td>Contributed Talk II: <strong> Efficient Bayesian Sampling Using Normalizing Flows to Assist Markov Chain Monte Carlo Methods </strong><br>
                <button type="button" class="collapsible">Abstract</button>
                <div class="content">
                  Normalizing flows can generate complex target distributions and thus show promise in many applications in Bayesian statistics as an alternative or complement to MCMC for sampling posteriors.
                  Since no data set from the target posterior distribution is available beforehand, the flow is typically trained using the reverse Kullback-Leibler (KL) divergence that only requires samples from a base distribution.
                  This strategy may perform poorly when the posterior is complicated and hard to sample with an untrained normalizing flow.
                  Here we explore a distinct training strategy, using the direct KL divergence as loss, in which samples from the posterior are generated by
                  (i) assisting a local MCMC algorithm on the posterior with a normalizing flow to accelerate its mixing rate and
                  (ii) using the data generated this way to train the flow. The method only requires a limited amount of <i>a priori</i> input about the posterior, and can be used to estimate the evidence required for model validation, as we illustrate on examples.
                </div>
              </tr>
              <tr>
                <td width="12%"><span id="event_13"></span></td>
                <td width="18%"></td>
                <td>Poster Spotlights II<br></br></td>
              </tr>
              <tr>
                <td><span id="event_14"></span></td>
                <td></td>
                <td>Poster Session II  <br>
                <a href="https://eventhosts.gather.town/0fH1RU147QI1cqPq/innf-poster-room-1" target="_blank">Poster Room 1</a> -
                <a href="https://eventhosts.gather.town/Q6X3qZkT5TMp3HPu/innf-poster-room-2" target="_blank">Poster Room 2</a> -
                <a href="https://docs.google.com/spreadsheets/d/1l1hA6IyEDLkzNMQuO2BLtsLWAI05dEC0R5lxNPJriMY/edit#gid=1514565630">Presenting papers</a></td>
              </tr>
            </tbody>
          </table>

        </div>

        <!-- <hr> -->

        <footer>
          &nbsp;
        </footer>

      </div>

      <!-- JavaScript -->
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
      <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
      </script>
      <script type="text/javascript" src="../js/main.js"></script>


    </div>
  </div>
</body>


<!-- Time -->
<script type="text/javascript" src="../schedule.js"></script>
<script>
  document.getElementById("timezone").innerHTML = Intl.DateTimeFormat().resolvedOptions().timeZone
  for (let i = 0; i < 15; i++) {
    // console.log(schedule[i][0])

    var event_start = new Date(schedule[i][0]);
    var event_end = new Date(schedule[i][1]);

    event_start = event_start.toLocaleTimeString([], {hour: '2-digit', minute:'2-digit', hour12: false})
    event_end = event_end.toLocaleTimeString([], {hour: '2-digit', minute:'2-digit', hour12: false})

    document.getElementById("event_"+i).innerHTML = event_start + ' - ' + event_end;
  }
</script>


<!-- JavaScript -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/javascript" src="../js/main.js"></script>

<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>




<script type="text/javascript"
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
MathJax.Hub.Config({
tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  displayMath: [['$$','$$']],
  processEscapes: true,
  processEnvironments: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
  TeX: { equationNumbers: { autoNumber: "AMS" },
       extensions: ["AMSmath.js", "AMSsymbols.js"] }
}
});
MathJax.Hub.Queue(function() {



  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});

MathJax.Hub.Config({

TeX: { equationNumbers: { autoNumber: "AMS" } }
});  </script>

</html>
